{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data:\n",
    "    def __init__(self, array, windowsize, offset   )\n",
    "    \n",
    "    \n",
    "    def shapeArray(self):\n",
    "        Input= np.array([self.array[i : i + self.windowsize] for i in range(len(self.array)-(self.windowsize + self.offset)+1)])   #part of data for training\n",
    "        Label= np.array([self.array[i + self.windowsize : i+ self.windowsize + self.offset] for i in range( len(self.x) - (self.windowsize + self.offset)+1)])  #label will be all of data?\n",
    "        return (Input,Label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def normalise(self):\n",
    "        \n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        norm_arr = scaler.fit_transform(         np.array( self.array ).reshape( np.array( self.array).shape[0] , 1 )        )\n",
    "        \n",
    "        return norm_arr\n",
    "    \n",
    "        \n",
    "    def unormalise(self):\n",
    "        min_og = min(self.array)\n",
    "        max_og = max(self.array)\n",
    "        \n",
    "        scaler2 = MinMaxScaler(   feature_range=(min_og, max_og)   )\n",
    "        unorm_array = scaler2.fit_transform(    array.reshape(    (self.array).shape[0], (self.array).shape[1]    )        )\n",
    "        \n",
    "        return unorm_array\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def network(shape, input_nuerons output_nuerons):\n",
    "        rnn = keras.models.Sequential()\n",
    "        rnn.add(keras.layers.LSTM(input_nuerons, input_shape=shape,return_sequences=True)) # LSTM layer with 20 neurons\n",
    "        rnn.add(keras.layers.Dense(output_nuerons,activation=\"linear\"))\n",
    "        rnn.compile(loss='mse',optimizer='adam')\n",
    "        \n",
    "        rnn.summary()\n",
    "        \n",
    "        \n",
    "    def train(self, steps, train_size):\n",
    "        costs=np.zeros(steps)\n",
    "        \n",
    "       \n",
    "        \n",
    "        for i in tqdm(range(steps)):\n",
    "            yInput, yLabel = shapeArray(   self.normalise()[0: train_size*len(  self.normalise()  )]    , self.windowsize, self.offset)\n",
    "            \n",
    "            #yInput=  np.array(ts_disx[0:int(0.3*len(ts_disx))])  # Apply the sinexp function to xInput\n",
    "            #yLabel=  np.array(ts_disx)  # Apply the sinexp function to xLabel\n",
    "            \n",
    "            # Now we need to reshape the array into batchsize * window size * features (1)\n",
    "            y_in=yInput.reshape(yInput.shape[0],yInput.shape[1],1)  # Reshape the input array to have the dimensions (batchsize,time samples, features)\n",
    "            # We only want a single number for the target (since we set return_sequences=False above). We will take the last sample of the yLabel\n",
    "            y_target = yLabel[:,-1].reshape(yLabel.shape[0], 1) # Reshape the output array to have the dimensions (batchsize, time samples)\n",
    "            \n",
    "            cost[i] = rnn.train_on_batch(y_in , y_target) #Train the network\n",
    "            \n",
    "            costs.append(cost)\n",
    "            \n",
    "        return costs\n",
    "            \n",
    "            \n",
    "    def predict(self):\n",
    "        yInput_test, yLabel_test = shapeArray(self.normalise(), self.windowsize, self.offset)\n",
    "        \n",
    "        yTest_in = yInput_test.reshape(yInput_test.shape[0], yInput_test.shape[1], 1)\n",
    "        \n",
    "        return y_Test = rnn.predict_on_batch(   yTest_in.reshape(yTest_in.shape[0], self.windowsize, 1)   )\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plots:\n",
    "    \n",
    "    def costplot(steps, costs ):\n",
    "        fig,ax=plt.subplots()\n",
    "        ax.plot(np.arange(steps),costs,label=r\"Costs\")\n",
    "        ax.set_xlabel(\"Steps\")\n",
    "        ax.set_ylabel(\"Cost\")\n",
    "        ax.set_title(\"Network Training Cost\")\n",
    "        ax.legend()\n",
    "        \n",
    "        \n",
    "    def pred_plot():\n",
    "        \n",
    "        fig,ax = plt.subplots()\n",
    "        ax.plot(ts_time,  ts_disy,'-o',label=\"Target\")\n",
    "        #ax.plot(x_test[offset:],y_testoff,label=r\"$y_{target}$\")\n",
    "        \n",
    "        #ax.plot(xLabel[0:15], train_label ,  '-o',label=\"Target\")\n",
    "\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"z displacement at a certain coordinate  (actual)\")\n",
    "        ax.set_title(\"Network Prediction\")\n",
    "        ax.legend()\n",
    "\n",
    "        #ax = ax.twinx()\n",
    "\n",
    "\n",
    "\n",
    "        ax.plot(ts_time[windowsize+windowsize-1:] , testPredict,  '-o'  , label=r\"$y_{prediction}$\")\n",
    "        #ax.set_ylabel(\"z displacement at a certain coordinate (predication)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
